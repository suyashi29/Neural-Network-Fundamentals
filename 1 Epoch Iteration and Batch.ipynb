{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c545938",
   "metadata": {},
   "source": [
    "### **Epoch vs Iteration in Neural Networks**\n",
    "\n",
    "Understanding the difference between **epoch** and **iteration** is crucial in machine learning model training.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Epoch**\n",
    "An **epoch** refers to **one complete pass** through the **entire training dataset**.\n",
    "\n",
    "- Suppose you have **10,000 data points** and decide to train your model for **5 epochs**. This means the model will see all 10,000 data points **five times**.\n",
    "- Increasing the number of epochs allows the model to learn better but may risk **overfitting** if the number is too high.\n",
    "\n",
    "ðŸ”¹ **Example:**  \n",
    "- Dataset size = 10,000 samples  \n",
    "- Batch size = 500  \n",
    "- **1 Epoch** = The model processes **all 10,000 samples** once.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Iteration**\n",
    "An **iteration** refers to **one update of model weights** based on a **single batch of data**.\n",
    "\n",
    "- In the above example:  \n",
    "- Dataset size = **10,000** samples  \n",
    "- Batch size = **500**  \n",
    "- In **one epoch**, the model will require:  \n",
    "\\[\n",
    "\\text{Iterations per epoch} = \\frac{\\text{Total Samples}}{\\text{Batch Size}} = \\frac{10,000}{500} = 20\n",
    "\\]\n",
    "\n",
    "So, in **one epoch**, there will be **20 iterations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Difference**\n",
    "\n",
    "| Aspect        | Epoch                                  | Iteration                            |\n",
    "|:---------------|:--------------------------------------|:-------------------------------------|\n",
    "| **Definition**  | One full pass through the entire dataset | One forward & backward pass over a single batch |\n",
    "| **Control**      | Controlled by specifying the number of epochs | Defined by batch size and dataset size |\n",
    "| **Example**       | 5 epochs = The model sees the full dataset 5 times | With batch size 500 and 10,000 data points, each epoch has 20 iterations |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Scenario**\n",
    "ðŸ’¬ **Suppose you train a model with the following settings:**  \n",
    "- **Dataset size:** 10,000 samples  \n",
    "- **Batch size:** 500  \n",
    "- **Epochs:** 3  \n",
    "\n",
    "âœ… **Total Iterations = 3 (epochs) Ã— 20 (iterations per epoch) = 60 iterations**\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Tip**\n",
    "- **More epochs** = More learning (but risk of overfitting).  \n",
    "- **More iterations** = Faster convergence (but may require tuning batch size).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2b979",
   "metadata": {},
   "source": [
    "### **Understanding Batch Size in Neural Networks**\n",
    "\n",
    "**Batch Size** is a crucial parameter in the training process of a neural network. It determines **how many samples are processed before the model's internal parameters (weights) are updated**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What is Batch Size?**\n",
    "- A **batch** is a subset of the total dataset used during one iteration of training.  \n",
    "- Instead of updating weights after processing the entire dataset, the model updates weights after processing **one batch**.  \n",
    "- Batch size directly impacts **memory consumption**, **training speed**, and **model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Types of Batch Sizes**\n",
    "There are three main approaches based on batch size:\n",
    "\n",
    "| Type                       | Description |\n",
    "|:----------------------------|:-------------|\n",
    "| **Batch Gradient Descent**  | Batch size = Entire dataset (slow but stable convergence).|\n",
    "| **Mini-Batch Gradient Descent** | Batch size = Small subset (commonly used in practice for optimal balance).|\n",
    "| **Stochastic Gradient Descent (SGD)** | Batch size = 1 sample (frequent updates, noisy convergence).|\n",
    "\n",
    "---\n",
    "\n",
    "### **3. How Batch Size Affects Training**\n",
    "| Batch Size | Effect on Training |\n",
    "|:------------|:-------------------|\n",
    "| **Small Batch Size (e.g., 16, 32)** | Faster updates but higher noise in gradient updates (less stable). |\n",
    "| **Medium Batch Size (e.g., 64, 128)** | Balanced convergence speed and stability (widely preferred). |\n",
    "| **Large Batch Size (e.g., 512, 1024)** | Slower updates but more stable convergence; may require more memory. |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Example Scenario**\n",
    "Imagine you have:  \n",
    "- **Dataset size:** 10,000 samples  \n",
    "- **Batch size:** 500  \n",
    "- **Epochs:** 3  \n",
    "\n",
    "âœ… **Total Iterations per Epoch** = \\( \\frac{\\text{Total Samples}}{\\text{Batch Size}} = \\frac{10,000}{500} = 20 \\)  \n",
    "âœ… **Total Iterations for 3 Epochs** = **3 Ã— 20 = 60 iterations**\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Choosing the Right Batch Size**\n",
    "- **Small Batch Size** â†’ Better generalization but slower convergence  \n",
    "- **Large Batch Size** â†’ Faster training but may lead to poor generalization  \n",
    "- **Recommended:** Start with **32** or **64**, and tune based on memory limits and model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Key Differences with Epochs and Iterations**\n",
    "| Aspect         | **Batch Size**                 | **Epoch**                     | **Iteration** |\n",
    "|:----------------|:-----------------------------|:-----------------------------|:-----------------|\n",
    "| **Definition**     | Number of samples processed before updating weights. | One complete pass through the dataset. | One weight update after processing a batch. |\n",
    "| **Control**         | Defined by the user during model setup.             | Controlled by the number of training cycles. | Defined by dataset size Ã· batch size. |\n",
    "| **Impact**           | Affects memory usage and training speed.            | Influences model's learning and potential overfitting. | Impacts the number of weight updates. |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Practical Tip**\n",
    "âœ… **Small batch size** for noisy data or better generalization.  \n",
    "âœ… **Larger batch size** for stable updates when GPU memory allows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7712a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
